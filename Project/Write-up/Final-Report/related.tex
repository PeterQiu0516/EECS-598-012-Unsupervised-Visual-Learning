
%%  3D from RGBs
% SfM (multi view, static scene) - video frames - correspondence 

% (multi view, static scene)
Structure from Motion is a well-studied field that reconstructs 3D structures based on 2D images. 
However, prior works~\cite{sinha2010multi, dani2011single, whelan2015elasticfusion, tamaazousti2011nonlinear, schonberger2016structure, schwarz1978estimating, ozden2010multibody} are limited to static scenes and cannot handle the occlusions caused by dynamic objects. 
Visual SLAM systems~\cite{whelan2015elasticfusion, raposo2016pi, raposo2013plane} capture consistent room-scale and surface-based maps using RGB-D images or videos. They find point or plane correspondences across frames and use geometry to predict the camera location of each frame and a static 3D pointcloud. Aside from using sensor depthmaps, they also assume certain parts of the environment remain unchanged. Our approach, on the other hand, aims to use RGB frames only and work on dynamic scenes.

% Structure from Motion (SfM) is a well-studied field that reconstruct 3D structures based on 2D image sequences. However, many prior works are limited to static scenarios and cannot handle the occlusion well caused by dynamic objects.  \cite{whelan2015elasticfusion} proposes a visual SLAM system that captures consistent room-scale and surface-based maps using RGB-D cameras. Its locally online optimization nature limits its application to static scenes. \cite{sinha2010multi} proposes a SfM technique using vanishing points to match image pairs, which puts constraints on the structure of the scene. \cite{dani2011single} introduces a reduced order observer model for SfM in the static environment. \cite{tamaazousti2011nonlinear} integrates prior knowledge about the scene into the inference of the camera location. Again, it assumes certain parts of the environment remain unchanged. 

% 3D consistency in video 
Recently, many works use deep learning methods to reconstruct 3D structure from images. There have been works to reconstruct normals~\cite{Eigen15,Wang15}, voxels~\cite{Choy20163d,Girdhar16b}, and depth~\cite{Eigen15,Ranftl2020} from 2D images. However, single view 3D cannot guarantee consistency across video frames and cannot handle dynamic scenes. There are also efforts to estimate consistent depth from video frames using temporal correspondences~\cite{karsch2014depth, patil2020don, wang2019recurrent, zhang2019exploiting} or geometric correspondences~\cite{Luo-VideoDepth-2020}. Our approach differs from these works by estimating planes instead of depth, which is a higher level representation. 

% PlaneRCNN PlaneNet (single view) * no correspondence
Our approach builds most heavily on works aiming to produce a planar reconstruction~\cite{liu2018planenet,yang2018recovering,liu2019planercnn,YuZLZG19,chen2020oasis,jiang2020peek}. In particular, we build on PlaneRCNN~\cite{liu2019planercnn}, which detects planes along with their geometric properties and instance masks from a single RGB image. Despite using surrounding frames to improve the detection quality, it only addresses the detection based on a single frame, and does not touch on the application when the temporal correspondence of the targeted image sequence needs to be learned. 

% SfM is also studied upon video frames. PlaneRCNN \cite{liu2019planercnn} detects planes along with their geometric properties and instance masks with a single RGB input image in the top-down fashion. Despite using surrounding frames to improve the detection quality, it only addresses the detection based on a single frame, and does not touch on the application when the temporal correspondence of the targeted image sequence needs to be learned. PlaneNet \cite{liu2018planenet} proposes a DNN for piece-wise planar depthmap reconstruction
% from a single RGB image and trains on the ScanNet video data. Similarly, it only focuses on geometry, depth and segmentation predictions and does not take advantage of the temporal correspondence in the video.

% Depth (multi view, dynamic scene) * no tracking? Yes; and depth is 2.5D

% Monocular depth estimation is an ill-posed problem which usually requires large amounts of data. However the datasets often have different limitations and biases. \cite{ranftl2020towards} tries to make use of diverse depth estimation data sources to train monocular depth estimation models, even though their annotations are incompatible. They approach the problem in a bottom-up way and design scale and shift invariant losses in the disparity space. To alleviate the dependence on the data, \cite{godard2019digging} applies self-supervised learning to predict the depth map.

\cite{CVPR2019_CycleTime, jabri2020walk} aims to learn useful visual representations for visual correspondence from raw videos without any supervision. Recently,~\cite{huang2021life} learns pixel-wise correspondence in a weakly-supervised approach by using epipolar constraints.
Our work builds on~\cite{jabri2020walk} which enforces strong affinity between image patches by learning embedding vectors to guide a random walk across a palindrome of frames. However, the original work suffers from very strict formulation to prevent the network from finding shortcuts during learning. Our approach adds plane normal aside from the embedding vectors to construct the affinity between nodes of adjacent frames. Our hope is that adding extra constraints can help the system to find robust planar correspondence across frames.
% \cite{jabri2020walk} aims to learn useful visual representations for video data that can improve performance on the downstream tasks. By viewing videos as a graph, they aim to enforce that the edges connecting two nodes (image patches) that have strong affinity have larger weights. They formulate the task as maximizing the probability of returning to the initial node in a random work on a graph that is constructed by a palindrome of frames. The probability of walking between two nodes is denoted as the softmax of the product of the affinity matrix between them. The local affinity map provides the possibility of detecting planes in the scene.

% In this work, we aim to compare the two approaches (top-down and bottom-up) to addressing the plane detection problem, while taking advantage of the feature map used for the random walk mentioned in \cite{jabri2020walk}. Unlike \cite{liu2019planercnn}, the temporal correspondence needs to be learned.